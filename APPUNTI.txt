
APPUNTI
Quando installi tensorflow su win7 il comando di cmd "tar -xf protoc-3.15.6-win64.zip" non funziona, perciò estrai noramlmente. 

Quando installi tensorflow su win7, non farlo se la dir ha un path troppo lungo

Euando esegui il comando per fare il training attenzione che il modulo "gin" di python deve corrispondere con la versione di tensorflow
	pip install gin-config==0.1.1

Elimina i checkpoint per runnare il codice G:\Bicocca\_Universita\Laurea Magistrale\Tesi magistrale\Tensorflow\workspace\models\my_ssd_mobnet

https://mediapipe.dev/

hand-gesture-recognition-using-mediapipe:
https://awesomeopensource.com/project/Kazuhito00/hand-gesture-recognition-using-mediapipe


i dati in ingresso devono essere maneggiati in modo tale che non soffrano dello scale e della posizione

il pugno come gesture non è buono

https://www.tecnobabele.com/android-non-si-connette-a-windows-tramite-adb-risolvilo-in-3-semplici-passaggi/2019-11-19/



moduloPrincipale = fullControll.py
	composto da:
	- def getKeyboardInput(me)
		che sfrutta keyPressModule con il quale ho un interfaccia di test per comandare il drone

	- main

		per eseguire il tracking:
			c'è una coda queue di lunghezza lenMaxQueue, + è grande + è chiaro il punto di partenza della sequenza di azioni, di contro + tempo nel partire a eseguire il tracciamento

			Quando la coda queue è piena dove a ogni cella troviamo (per ora...***) la somma delle coordinate x e y del palmo (key 0: WRIST) di lmList, ovvero la lista che contiene tutti gli hand landmark (mostrare immagine), allora si passa dallo stato INIZIALIZATION allo stato START. Si esegue la media m su tutti i valori della coda queue. Se m non è troppo lontano dal nuovo valore del palmo (tolleranceSTART) allora si passa allo stato TRACKING. 

			Nella fase di tracking eseguo la media m2, ma questa volta non su tutti gli elementi della coda, ma solo sugli ultimi n elementi (nlastMovements, attualmente settato a 5). Il perché è dovuto al fatto che ci stiamo muovendo e vogliamo che il movimento sia fatto piano di una certa tolleranza (tolleranceTRACKING), se così non fosse ritorniamo allo stato START. 

			Stavo pensando di implementare uno stato in caso di una momentanea perdita del tracciamento, di recuperare il tracking + velocemente. Attualmente ho come l'impressione che certe volte lo faccia, presumo per via dei punti vicini.

			***Penso di migliorare tale sistema facendolo per tutti o quasi tutti i riferimenti della mano, ed eventualmente fare media x e media y, non media della somma delle coordinate. 

		riconoscimento gesture:
			getDataHandGesture.py, modulo dinamico per poter creare modelli con cui riconoscere più gesti
				gesti attualmente riconosciuti = ['stop', 'onefingerup', 'twofingerup', 'thumbsup']

			number_imgs = 50, + è grande + dati per ogni categoria

			possiamo recuperare i dati sia con il tello che tramite webcam. Il tello però ha un problema, in quanto si spegne dopo un utilizzo solo di camera senza decollo. Ho gestito tale problema con un state.json

			se individuata una mano si recuperano i punti di riferimento, 42 (21 per le coordinate x e altri 21 per le coordinate y). Questi vengono poi traslati all'origine, (mediando sui punti e sottraendo tale media a ciascun riferimento rispettivamente per coordinate x e y). Ho intenzione anche di mediare tutto su 1, attualmente non viene fatto, ma funziona bene lo stesso. 

			handgesturerecognition.ipynb
			qui viene passato il csv che ho generato al passo precedente. Viene creato un classificatore DNN (quindi, deep neural network). Mi sono limitato a due layer nascosti di 30 e 10 nodi rispettivamente, con 4 classi

			Con 10 imgs di training per categoria mi era uscito 0.85 di accurancy. Spostando a 50 imgs accurancy = 1.
				global step 5000: accuracy = 1.0, average_loss = 0.018036956, global_step = 5000, loss = 0.018036956


			con handGestureRecognition.py recupero il modello sviluppato nel passo precedente e lo uso in fullControll.py.
			eseguo anche il drawHandGesture() che si adatta alla mano e riporta il nome del gesto visualmente.


Approfondimenti:
- in traininganddetection.ipynb ho creato un modulo per eseguire object_detection in real time. Sfrutto un pretrained model: ssd_mobilenet_v2. Allo stato attuale potrebbe non servire.

-numba: Python code 1000x Faster

handTrackingBasics.py, test.py, tello.py, main.py sono moduli di test da sistemare/cancellare


Sviluppi futuri:
- testare il tutto sul drone

- unreal engine o unity per creare applicazione? oppure kivy? Preferirei forse unity perché ho già esperienza, e magari chissà riesco a collegarci google arcore per creare un ambiente virtuale. Creazione di un server o calcolo computazionale su smartphone?

- prelevare le traettorie tramite app

- aggiungere più gesti di cattura

Step:
1) specifica/documento di quello che voglio ottenere. 
2) acquisizione traettorie limitata 2d e/o 3d (forse si può ottenere un coefficiente di scalatura dal frame A al frame B)
3) disegnare la trajectory, con un certo profilo di velocità (colori a indicare la variazione della velocità nel tempo t). 
4) Raggiungere il risultato di almeno una simulazione (persona e drone) su Gazebo. Racconto una traettoria. 
TELLO ha i suoi nodi su ros. Verificare che queste repo per il tello in ros siano validi. http://wiki.ros.org/tello_driver
Col franca abbiamo più esperienza. Una via di mezzo tra algoritmo di controllo standard e algo di trajectory? Non mi aspetto che venga seguita alla perfezione la traettoria.

Impo:
Iniziare a scrivere le implementazioni.
Questa parte della generazione delle traettorie 3d, mediapipe e riconoscimento dei gesti si può iniziare a scrivere.
E' più interessante fare una buona simulazione (ed eventalmente una presentazione pratica) che tante cose a metà.


FATTO:
- dive opportunity to create or not the log3dtrajectory
- fix timer duration trajectory
-non tutte le palle su ros si eliminano

DA FARE:
-mgliorare framerate su ubuntu


Per oggi:
8/1
-[DONE]stabilire un titolo e inviarlo ai prof per conferma
-[DONE] aggiungere battery nel display
-[DONE]sistemare abstract e altre parti e inviare pdf ai prof
-[DONE]contattare il prof ciocca, aggiornandolo sulla data di quando mi voglio laureare, informarlo degli avanzamenti e chiedere anche a lui una lettura/consigli della tesi nei prossimi giorni.
-[DONE]informare anche i prof giusti e loris che ho risentito il mio relatore della bicocca

9/1
-[DONE]inserire commenti al codice
-[DONE]rendere più chiaro il main
-[DONE]dare in input al drone la traettoria eseguirla e andare a provarla

10/1
-[DONE]guardare parte andrew ng sulle NN
-[DONE] iniziare scrivere nella tesi parte relativa alle reti neurali

11/01
-[DONE] scrivere nella tesi parte relativa alle reti neurali
-[DONE][magari è meglio aspettare prima riposta prof] scrivere nella tesi parte relativa al batch gradient descent per la ridge regression 


17/01
-[DONE]skip n step dovrebbe essere eseguito semplicemenete salvando il dato ogni n secondi, ora invece salva una montagna di dati e ogni secondo vengono recuperati solo i valori di n secondi

21/01/2022
-[Done] scale text log with resolution
-[DONE]controllare il risultato con adam ottimizzatore
-[DONE]Leaky Rectified linear activation 

-aggiornare pipeline per comandi veloci di controllo con la mano
-creare una scena di acquisizione video nella simulazione
-nella simulazione aggiungere anche lo yaw (NON pitch e il roll)
-ritornare dizionario di valori invece di n liste...(magari robe in numpy)
-aggiungere gesto di "pugno", "left", "right" (vedere se funziona)
4.	Non e’ chiaro se nell’acquisire la traiettoria consideri anche la componente temporale, o se e’ solo un path che poi segui a velocita’ costante.
5.	Idem per lo yaw del drone.  Guarda sempre nella stessa direzione?


nell'evaluation mettiamo i risultati della rete neurale, come mappa l'univariate spline. Che metrica invece per il mappaggio della traiettoria nel real?

diegnare la rete neurale con qualche tool
https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.70351&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false

23/01/2022
-[DONE] cattura solo mano sinistra
-[DONE] creare cartella dove viene inserito video del drone, video della camera, dati originali, dati normalizzati, foto delle traiettorie dai dati normalizzati e foto delle traiettorie effettive

29/01/2022
[DONE] aggiunto stima depth sfruttando stima orientamento. ATTENZIONE bisogna assolutamente iniziare con la mano perfettamente perpendicolare rispetto alla camera, altrimenti al momento problemi.
the rotation matrix: testare con le generiche matrici di rotazione.


quando specifici:
was 
had

in passsato sei stato
I have been at the university

to estimate = sia nome che verbo

30/01
[done] implementato ridge regression. Spline interpolation also avoids the problem of Runge's phenomenon, in which oscillation can occur between points when interpolating using high-degree polynomials. Stesso discorso riguarda la ridge regression



05/02/2021
[done] salvare immagine prima e dopo traiettoria smussata
[done] rimuovere da git immagine della traiettoria e rimuovere video src

catturare solo il primo frame della mano quando questa è simbolo detect (serve per il roll, pitch and yaw)


#https://www.pluralsight.com/guides/linear-lasso-ridge-regression-scikit-learn
#https://www.statology.org/adjusted-r-squared-in-python/
#https://towardsdatascience.com/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b
#https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce
#https://www.kaggle.com/residentmario/model-fit-metrics/notebook
# for 1 gesto con k gradi diversi e salvare r^2, MSE e RMSE

fixare immagine output