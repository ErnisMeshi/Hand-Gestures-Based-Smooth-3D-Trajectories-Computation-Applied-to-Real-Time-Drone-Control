\setcounter{chapter}{0}
\chapter{Literature Review}
\label{chap:stateoftheart}

\glsreset{ai}
\glsreset{dl}
\glsreset{dnn}

While being natural to people, robust real-time hand perception is a decidedly challenging computer vision task, as hands often occlude themselves or each other and lack high contrast patterns. MediaPipe \cite[]{lugaresi2019mediapipe} offers cross-platform and customized ML solutions for live and streaming media, and among these there is one that deals with the task of finger/hand tracking. Since this solution was used at the basis of the built system, the main elements that characterize their classifier will be described. In this chapter, the section is divided into two parts: in the first one the architecture of the system will be described, while the second will focus on the results obtained. \\

\noindent Mediapipe proposes an efficient hand tracking pipeline that can track multiple hands, and a hand pose estimation model that is capable of predicting 2.5D hand landmarks with only \gls{rgb} image input \cite[]{zhang2020mediapipe}. The solution does not require any additional hardware and performs in real-time on mobile devices. \\

\noindent The above mentioned hand tracking solution utilizes a ML pipeline consisting of a palm detector and a hand landmark model. The first one operates on a full input image to locate palms via an oriented hand bounding box. The second one, operates on the cropped hand bounding box provided by the palm detector returning high-fidelity 2.5D landmarks. Providing the accurately cropped palm image to the hand landmark model drastically reduces the need for data augmentation (e.g. rotations, translation and scale); furthermore, it allows the network to dedicate most of its capacity towards landmark localization accuracy. In a real-time tracking scenario, the bounding box has been derived from the landmark prediction of the previous frame as input for the current frame, thus avoiding applying the detector on every frame. Instead, the detector is only applied on the first frame, or when the hand prediction indicates that the hand is lost. \\

\noindent To detect initial hand locations, a single shot detector model optimized for mobile real-time application has been deployed. This model has to work across a variety of hand sizes with a large scale span and be able to detect occluded and self-occluded hands. First, instead of a hand detector a palm detector has been trained, because estimating bounding boxes of rigid objects like palms and fists is significantly simpler than detecting hands with articulated fingers. Second, an encoder-decoder feature extractor has been used  for a larger scene-context awareness even for small objects. Lastly, the focal loss has been minimized: the focal loss is an extension of the cross-entropy loss function that would down-weight easy examples and focus training on hard negative, used mainly for dense object detection.

\begin{figure}[H]
	\centering
	\includegraphics[width=.5\textwidth]{images/mediapipepalmdetec}
	\caption[Palm detector model architecture.]{Palm detector model architecture.}
	\label{fig:mediapipepalmdetec}
\end{figure}

\noindent After running palm detection over the whole image, the subsequent hand landmark model performs precise landmark localization of 21 2.5D coordinates inside the detected hand regions via regression. The model learns a consistent internal hand pose representation and is robust even to partially visible hands and self-occlusions.

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/mediapipehandlandmark}
	\caption[Architecture of hand landmark model..]{Architecture of hand landmark model.}
	\label{fig:mediapipehandlandmark}
\end{figure}

\noindent The model has three outputs: 

\begin{itemize}
  \item 21 hand landmarks consisting of x, y, and relative depth (note that depth was not used for this project). The 2D coordinates are learned from both real-world images as well as synthetic datasets;
  \item A hand flag indicating the probability of hand presence in the input image;
  \item A binary classification of handedness, e.g. left or right hand.
\end{itemize}

\noindent For the hand landmark model, the experiments showed that the combination of real-world and synthetic datasets provided the best results. Only real-world images have been evaluated. The target was to achieve real-time performance on mobile devices, therefore the experiment was done with different model sizes; it has been found that the “Full” model (see Table \ref{fig:mediapiperes}) provided a good trade-off between quality and speed. Increasing model capacity further introduced only minor improvements in quality but decreased significantly in speed.

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/mediapiperes}
	\caption[Hand landmark model performance characteristics.]{Hand landmark model performance characteristics.}
	\label{fig:mediapiperes}
\end{figure}

\bigskip

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse

Siccome non è mai esistito un sistema che riconosce la mano davvero efficiente e in tmepo reale allora non è mai esistito la possibilità di poter controllare le traiettorie con la mano. In seguito alla soluzione recente fatta dai ricercatori di google che ha risolto i diversi problemi di performance e precisione nel riconoscere la mano, ora è possibile ragionare su come eseguire delle traiettorie con la mano. QUesto mio studio ha l'obiettivo quindi di aggiungere un'ulteriore mattoncino al lavoro svolto dai ragazzi di google per controllare il drone usando soltanto la mano, risolvendo quindi diversi problemi.

%%%%%%%%%%%%%%%%%%%%%%
\fi