Robotic systems are increasingly being adopted for filming and photography purposes. In fact, by exploiting robots, it is possible to program complex motions, achieving high-quality videos and photographs. Our contribution is to propose an alternative to the joystick: being able to control a drone using just a hand and giving space to new ways of shooting video content, even to those who are novices. To achieve this, after constructing a Deep Neural Network, which recognizes the gestures of a hand, and later creating a solid pipeline, which detect 3D trajectories obtained from 2D reference points, we captured 3D movements using a state-of-the-art hand tracking system. We estimated the orientation of the hand, and, with this information, we estimated depth as well. 3D trajectories were interpolated and noise-purified with 3 Ridge Regressions. Firstly, as a proof of concept, the captured trajectory was launched in a simulation on a drone, implemented in the ROS framework, and later in a real drone called DJI Ryze Tello. The whole pipeline can be easily translated into another kind of task, e.g., interaction and communication in AR / VR.