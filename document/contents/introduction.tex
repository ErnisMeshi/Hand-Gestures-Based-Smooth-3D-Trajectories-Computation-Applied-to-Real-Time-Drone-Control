\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{}{}
\label{chap:intro}

% INSERIRE IMMAGINE
% FARE IN NERETTO DOVE C'è DA FARE LISTE

% https://roboticsandautomationnews.com/2021/12/30/robots-increasingly-taking-over-film-camera-work/48012/
Robots are increasingly taking over from what has traditional been the preserve of human creative people. Film creators are progressively turning to automated systems for recording moving images. Producing new, targeted, and engaging content can be a stressful, time-consuming endeavor. This is why more and more creators and advertisers are embracing tools that take the sting out of the process. Cinema robot not only decreases production time, but it allows teams to shoot a variety of angles that would be physically impossible or cost a fortune in labor. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/sisujoy}
	\caption[Guide a robot with the motion of the hand using a joystick.]{Guide a robot with the motion of the hand using a joystick.}
	\label{fig:drnsisu}
\end{figure}

% https://www.youtube.com/watch?v=CdOaGU04MyM&t=232s
\noindent Unfortunately, it is known that robot programming may be a very laborious task. They have to synchronize robot motion with objects in the frame. There are also other requirements that need to be taken into account, such as camera settings (focal length, aperture time and desired depth of field). The credit for trying to speed up the process of robot programming goes to different companies. For example, KUKA, a partner of andyRobot, has developed a plug-in for industry leading animation software, Autodesk Maya, that allows KUKA robots to be programmed by anyone who knows how to animate inside Maya \cite[]{Animatin29:online}: a robot program can be created by simply dragging the \gls{3d} model of the robot through space in the virtual world and setting keyframes. Furthermore, SISU Cinema Robotics can be named: the company has developed a way to guide the robot with the motion of the hand using a joystick \cite[]{SISUCine24:online} (see Fig. \ref{fig:drnsisu}). \\

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/drn}
	\caption[Drones in filmmaking.]{Drones in filmmaking.}
	\label{fig:drnfilm}
\end{figure}

% https://dakona.co.uk/top-tips-for-drone-cinematography/
\noindent Drone filming and aerial photography (see Fig. \ref{fig:drnfilm}) present a wide range of benefits and opportunities to businesses and brands looking to capture stunning aerial imagery and to produce incredible new perspectives \cite[]{TopTipsf86:online}. There can be quite a steep initial learning curve when it comes to getting to grips with drone technology. Furthermore, to capture high-quality content, it is important to use equipment that is both easy to operate and correctly set up as it is also important to understand how to fly smoothly and consistently capture professional and smooth looking aerial footage. In fact, not every position of the drone are visually interesting angles, as sometimes it is about understanding composition and depth that makes all of the difference. For the success of the video shooting, it is extremely crucial to look for a specific angle that seems unique, cinematic, and suggestive that will hold the attention of the audience. A lot of drones have additional features that help pilots capture great looking footage, such as centre points that help frame shots. Additionally, filters can be added to certain drone cameras, allowing them to control the amount of light entering the lens without compensating with shutter speed. \\

\noindent Our goal is being able to control a drone using just the motion of a hand for shooting, without the use of a joystick. The system can also be used to program a trajectory easily. This can be very interesting because it is an intuitive system: basically, no experience is required in piloting. In addition, the one who shoots could potentially also be the actor. In spite of that, ease of use does not imply that the system does not require attention: it is quite simple to get caught up in the task at hand, and completely lose track of the flight time. Some drones, such as DJI, will begin automatically landing when the battery begins to get low, however there are still a lot of other drones that do not have this intelligence and will literally crash out of the sky when it runs out of battery. Most crashes are actually caused by battery voltage drop \cite[]{WhatHapp70:online}. Not moderating the flight time and not keeping a close eye on the battery levels, can lead to disastrous accidents \cite[]{ListofUA81:online}. \\

\noindent In order to achieve this goal, research has been carried out in the literature to see if a hand tracking system already exists. There is some solution such as \cite[]{oikonomidis2011efficient} where, with the extensive experiments with a prototype GPU-based implementation of the proposed method, demonstrates that accurate and robust \gls{3d} tracking of hand articulations can be achieved in near real-time (15Hz), using a Kinect sensor. A more recent work \cite[]{OculusCo32:online} marking another important \gls{vr} input milestone, in the evolution of \gls{vr} input, is the announcement of hand tracking on Oculus Quest enabling natural interaction in using hands on an all-in-one standalone device. All without the need for a controller, external sensors, gloves, or a PC to power it. And finally, one of the latest research of Google Group research \cite[]{zhang2020mediapipe}: a solution that does not require any additional hardware and performs in real-time on mobile devices (the one actually used in this project and widely spoken in the literature chapter \ref{chap:stateoftheart}). Thanks to this system it was possible to build a \gls{dnn} to recognize specific hand gestures. The orientation of a specific gesture (called "detect gesture") was also estimated in order to obtain also information on depth. Through hand gesture recognition and orientation estimation, a pipeline to detect a \gls{3d} trajectory was defined. However, this trajectory $G()$ was disturbed by estimation errors. Hence, two different data fitting algorithms were applied on the trajectory $G()$: first $3$ splines and then $3$ ridge regression, in both cases for each component $X$,$Y$ and $Z$ of $G()$. This phase allowed to go from trajectory $G()$ to $G_{smooth}()$. \\

\noindent As a \gls{poc}, the captured trajectory was launched in simulation on a drone, implemented in the \gls{ros} framework. After that, the DJI Ryze Tello drone (see Sec. \ref{subsec:tello}) was used to test the application in real life. DJITelloPy is the DJI Tello drone python interface used in the project to make the application communicating with the drone. The trajectory veracity was visible to the naked eye. The correctness of the fixed dimension of the trajectory was extremely important for the project’s purpose since, if the range of action in which the drone operates is uncontrolled, then it could get dangerous for those around it. \\

\noindent The project's objective was reached with success and the entire pipeline for the acquisition of the trajectory can be exploited in \gls{ar} and \gls{vr} scenarios as it would allow people to interact with virtual objects and/or perform actions.