\chapter{Background}
\label{chap:background}
\glsreset{nn}
%FIXME sections
This chapter provides some background concepts to understand the material 
presented in this thesis. 

%Nel caso la tesi riguardi un progetto di sviluppo software,
%è preferibile, per maggiore chiarezza, descrivere i requisiti, l’interfaccia utente e
%l’architettura in capitoli separati (il codice potrà essere allegato in appendice). Il capitolo
%finale dovrà contenere una sintesi del lavoro, e una descrizione degli eventuali problemi
%aperti e dei possibili sviluppi futuri.

\section{Regression}
% Bishop -pattern recognition and machine learning (pg. 137)
% andrew ng - Da pag 3 fino a pag 11 (cioè Polynomial Regression)
The goal of regression is to predict the value of one or more continuous target variables $t$ given the value of a $D$-dimensional vector $x$ of input variables. \cite[]{bishop:2006:PRML}

\noindent Given a training data set comprising $N$ observations ${x_n}$, where $n = 1,...,N$,
together with corresponding target values ${t_n}$, the goal is to predict the value of $t$
for a new value of $x$.

\noindent From a probabilistic perspective, we aim to model the predictive distribution $p(t|x)$ because this expresses our uncertainty about the value of $t$ for each value of $x$.

\subsection{Linear Models for Regression}
\label{subsec:reglinuniv}

The simplest linear model for regression is one that involved a linear combination of the input variables:
\begin{Equation}[H]
	\centering
	\begin{equation}
	y(x,w)=w_0 + w_1x_1+...w_D x_D
	\end{equation}
	\label{eq:mathmodela}
\end{Equation}

\noindent where $x=(x_1,...,x_D)^T$. This is often simply known as linear regression. The key property of this model is that it is a linear function of the parameters $w_i$, but also of $x_i$ and establishes significant limitations on the model. It is possibile extend the class of models by considering linear combinatioins of fixed nonlinear functions of the input variables:
\begin{Equation}[H]
	\centering
	\begin{equation}
	y(x,w)=w_0 + \sum_{j=1}^{M-1}w_j \phi_j(x)
	\end{equation}
	\label{eq:mathmodela}
\end{Equation}

\noindent where $\phi_j(x)$ are known as basis functions. The total number of parameters in this model will be $M$.

\noindent The parameter $w_0$ is called bias parameter. It is often convenient to define an additional dummy "basis function" $\phi_0(x)=1$, so that:

\begin{Equation}[H]
	\centering
	\begin{equation}
	y(x,w)=\sum_{j=0}^{M-1}w_j \phi_j(x) = w^T\phi(x)
	\end{equation}
	\label{eq:mathmodela}
\end{Equation}

\noindent where $w=(w_0,...,w_{M-1})$ and $\phi=(\phi_0,...,\phi_{M-1})^T$

\noindent By using non linear basis functions, we allow the function y(x,w) to be a non linear function of the input vector x.  A particle example of this model where there is a single input variable $x$ is the polynomial regression. The basis functions take the form of powers of $x$ so that $\phi_j(x)=x^j$.

\noindent There are other possibile choices for the basis functions as:

\begin{Equation}[H]
	\centering
	\begin{equation}
		\phi_j(x)=e^{\frac{-(x-u_j)^2}{2s^2}}
	\end{equation}
	\label{eq:mathmodela}
\end{Equation}

\noindent where $u_j$ regulates the locations of the basis functions in input space, while the parameter $s$ is their spatial scale. These are usally reffered to as "Gaussian" basis functions. 

\noindent We can use simply the identity basis functions in which the vector $\phi(x)=x$.

\subsection{Normal equations}
\label{subsec:reglinmulnormeq}
% Bishop -pattern recognition and machine learning (pg. 25 intro chap 1.1)
%https://en.wikipedia.org/wiki/Mean_squared_error
The values of the coefficients will be determined by fitting the polynomial to the training data. This can be done by minimizing an error function that measures the misfit between the function $y(x, w)$, for any given value of $w$, and the training set data points. One simple choice of error function, which is widely used, is given by the sum of the squares of the errors between the predictions $y(x_n, w)$ for each data point $x_n$ and the corresponding target values $t_n$ (called also sum of squares regression \cite[]{sum-squares}), so that we minimize:

\begin{Equation}[H]
	\centering
	\begin{equation}\label{eq:mats}
		E(w)=\frac{1}{2} \sum_{n=1}^{N} [y(x_n,w)-t_n]^2
	\end{equation}
\end{Equation}

\noindent where the factor of 1/2 is included for mathematical convenience. It is a nonnegative quantity that would be zero if, and only if, the function $y(x, w)$ were to pass exactly through each training data point.

\noindent We can solve the curve fitting problem by choosing the value of $w$ for which $E(w)$ is as small as possible. Because the error function is a quadratic function of the coefficients $w$, its derivatives with respect to the coefficients will be linear in the elements of $w$, and so the minimization of the error function has a unique solution $w^*$. The resulting polynomial is given by the function $y(x, w^*)$.

% Leggere gli articoli che chiariscono differenza tra MSE e RSS...
%https://stats.stackexchange.com/questions/73540/what-is-the-relationship-between-the-mean-squared-error-and-the-residual-sum-of
%https://365datascience.com/tutorials/statistics-tutorials/sum-squares/
%https://en.wikipedia.org/wiki/Errors_and_residuals#Regressions
\noindent We can  write (\ref{eq:mats}) in matrix notation as:

\begin{Equation}[H]
	\centering
	\begin{equation}
	\frac{1}{2} (\phi w - t)^T (\phi w - t)
	\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent where $\phi$ is an $N x M$ matrix, so that:
 
% https://jasonwarta.github.io/latex-matrix/ 

\begin{Equation}[!htb]
	\centering
	\begin{equation}
	\phi =
		\begin{pmatrix}
			\phi_0(x_1) & \phi_1(x_1) & \dots & \phi_{M-1}(x_1) \\
			\phi_0(x_2) & \phi_1(x_1) & \dots & \phi_{M-1}(x_1) \\
			\vdots & \vdots & \ddots & \vdots \\
			\phi_0(x_N) & \phi_1(x_N) & \dots & \phi_{M-1}(x_N) \\
		\end{pmatrix}
	\end{equation}
	\caption[Design matrix.]{This is called the design matrix whose elements are given by $\phi_{nj} = \phi_j(x_n)$.}
	\label{eq:hommatrix}
\end{Equation}

\begin{Equation}[!htb]
	\centering
	\begin{equation}
	w = 
	\begin{pmatrix}
	w_0 \\
	\vdots \\
	w_{M-1}
	\end{pmatrix}
	\quad t =
	\begin{pmatrix}
	t_1 \\
	\vdots \\
	t_N \\
	\end{pmatrix}
	\end{equation}
	\label{eq:hommatrix}
\end{Equation}

\noindent Therefore, we have to solve the optimization problem finding the minimum of the cost function $E(w)$:
\begin{Equation}[H]
	\centering
	\begin{equation}
	w^*= \operatorname*{arg\,min}_w  \frac{1}{2}(\phi w - t)^T (\phi w - t)
	\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent So we compute the gradient and solving for $w$ we obtain:
\begin{Equation}[H]
	\centering
	\begin{equation}
		\begin{aligned}
			\nabla E(w) = \phi^T(\phi w - t) &= 0 \\
			\phi^T \phi w - \phi^T &= 0 \\
			\phi^T \phi w &= \phi^T t \\
			w^* &= (\phi^T \phi)^{-1} \phi^T t
		\end{aligned}
	\end{equation}
	\caption[Normal equations.]{They are known as the normal equations for the least squares problem}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent This is the real minimum because if we take the second derivative $ \nabla^2 E(w) = \phi^T \phi $ and this is a symmetric matrix, so it's also positive definitive matrix, which means that this objective function that we try to minimize is convex. So if we find a stationary point, such that derivative is zero, we also find a global minium. Furthermore, to find a solution we need to invert this matrix $ \phi^T \phi $, so we need some condition that assure this is invertible and this is the case when the columns of the matrix are linearly independent. 

\noindent Once we find the solution $w^*$ and we receive a new data point that we never seen during training, we just predict the new target $t^*$ as:

\begin{Equation}[H]
	\centering
	\begin{equation}
	\begin{aligned}
	t^* = w^{*^T} \phi(x)
	\end{aligned}
	\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\section{Regularization}
\label{subsec:poverfitting}
If we use a set of features that is too expressive, for example a ten polynomial grade ($x^{10}$), then this interpolates very close our training data, because we have a model with 10 parameters where we can perfectly represents the data points. The risk is that the model became something like this:

\noindent[picture]

\noindent We want a trade-off between fits data and able to generalize. Infact, on the other hand if our model is not to expressive and not to complex our data will be linearly rapresentable in the feature space and this means that the performance will be very poor. 

\subsection{The Problem Of Overfitting}
We want to penalize for some features with parameters with high values, if our model is overfitting is very likely that the parameters of our model will have a big magnitude, this means that also the features supply to this parameters will be higher and very low and this cause a lot of problems.

\noindent In order to control over-fitting we introduce the idea of adding a regularization term to an error fuction, so that the total error function to minimized takes the form:

\begin{Equation}[H]
	\centering
		\begin{equation}
			\begin{aligned}
				E(w) = E_D(w) + \lambda E_W(w)
			\end{aligned}
		\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent Where $\lambda$ is the regularization coefficient and it is the trade-off between how we well fit training set and how to establish the parameters $w$ with low values, therefore having simple hypotesys avoiding over-fitting. $E_D$ is the error based on dataset, while $E_W$ is based on weights.

\subsection{Cost Function}
\label{subsec:regcostfun}

One of the simplest forms of regularizer is given by the sum-of-squares of the weight vector elements:
\begin{Equation}[H]
	\centering
		\begin{equation}
			\begin{aligned}
				E_W(w) = \frac{1}{2} w^T w = \frac{||w||^2_2}{2}
			\end{aligned}
		\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent where $||w||_2$ is the euclidean norm $\sqrt{ \sum_{i=1}^{n} x^2_i}$.

%https://en.wikipedia.org/wiki/Ridge_regression
\noindent This is also called ridge regression, a method of estimating the coefficitents of multiple-regression models in scenarios where indipendent variables are highly correlated.

\noindent If we also consider the sum-of-squares error function given by:
\begin{Equation}[H]
	\centering
		\begin{equation}
			\begin{aligned}
				E_D(w) = \frac{1}{2} [t_n - w^T \phi(x_n)]^2
			\end{aligned}
		\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent then the total error function becomes:

\begin{Equation}[H]
	\centering
		\begin{equation}
			\begin{aligned}
				E(w) = \frac{1}{2} \sum_{n=1}^{N}[t_n - w^T \phi(x_n)]^2 + \frac{\lambda}{2} w^T w
			\end{aligned}
		\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent This particular choice of regularizer is known in the machine learning literature as
weight decay because in sequential learning algorithms, it encourages weight values
to decay towards zero.

\noindent Setting the gradient of $E(w)$ wrt $w$ to zero, and solving for $w$, we obtain:

\begin{Equation}[H]
	\centering
		\begin{equation}
			\begin{aligned}
				w^* = (\lambda I + \phi^T \phi)^{-1} \phi^T t
			\end{aligned}
		\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent and this is an extension of the least-squares solution (2.10). This is a better version than before because is also possibile prove that ($\lambda I + \phi^T \phi$) is always invertible if $\lambda > 0$, therefore $w^*$ always exists.

\subsection{(Batch) Gradient Descent}
\label{subsec:batchgradientdescen}




\section{Artificial Neural Networks}
\label{sec:nn}
The term  \gls{nn} has its origins in attempts to find mathematical representations of information processing in biological systems. In fact, \glspl{ann}, subset of \gls{ml} field, are models inspired from the biological performance of human brain \cite[]{inbook}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/Neuron3}
	\caption[Image of a human neuron.]{Neuron and myelinated axon, with signal flow from inputs at dendrites to outputs at axon terminals. The signal represents a short electrical pulse called 'spike'.}
	\label{fig:bioneuron}
\end{figure}

\noindent Neurons have cel body (fig. \ref{fig:bioneuron}) and a number of input wires called dendrites. Neurons also have an output wire called axon, used to send signals to other neurons. At a simplistic level the neuron is a computational unit that gets a number of inputs through its input wires, then does some computation and finally it sends outputs to other neurons connected to it in the brain. \\

\subsection{Feedforward Fully-Connected Neural Networks}
\label{nn:feedforward}

\noindent In (fig. \ref{fig:nn}) is visible a \gls{nn}, seen as mathematical model. It's just a group of this different neurons strung together. Trying to underline an analogy with the biological systems: circles identify the cell body where they are feeded with some inputs that pass through the input wires, similar to the dendrites. The neuron does some computation and outputs some value on an output wire, where in the biological neuron it identifies the axon.

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/NN}
	\caption[Feed forward neural network.]{The image graphically shows a \gls{nn} with three layers (or two hidden layers). The input layer has $M^{(0)}$ neurons and the output layer has $K$ outputs. Neurons are organized in  layers to allow parallel copmutation to avoid cyclic dependendices. The process of computing \gls{nn} outputs from inputs is called forward propagation. This kind of network is also called Multi-layer perceptron.}
	\label{fig:nn}
\end{figure}

\noindent The $z^{(l)}_m$ are neurons, each takes its input values and computes a single output value from them. Neurons are organized in layers $1,...,L$ and usually the starting input is considered the 0th layer. Inputs $x_1,...,x_D$ are occasionally called input layer/neurons (even though they do not compute anything). The output of the entire network is then $y=z^{(L)}$, called output layer. Instead, the internal layers are called hidden layers. Each layer $l\in \{1,...,L\}$ has $M^{(l)}$ neurons.

\noindent $M^{(1)}$ neurons perform a preceptron-like computation:

\begin{Equation}[H]
	\centering
	\begin{equation}
	u^{(1)}_m = (w^{(1)}_m)^T x + b^{(1)}_m,  
	\quad \quad
	z^{(1)}_m = f(u{(1)}_m),
	\quad \quad
	m=1,...,M^{(1)}
	\end{equation}
	\label{eq:hommatrix}
\end{Equation}

\noindent with a differentiable activation function $f$ for gradient descent. This step is iterated multiple times taking the outputs of the previous step:

\begin{Equation}[H]
	\centering
	\begin{equation}
	z^{(l-1)} = (z^{(l-1)}_m)_{m=1,...,M^{(l-1)}}
	\end{equation}
	\label{eq:hommatrix}
\end{Equation}

\noindent as input of:

\begin{Equation}[H]
	\centering
	\begin{equation}
		u^{(l)}_m = (w^{(l)}_m)^T z^{(l-1)} + b^{(l)}_m,  
		\quad \quad
		z^{(l)}_m = f(u^{(l)}_m)
	\end{equation}
	\label{eq:hommatrix}
\end{Equation}

\noindent where $m=1,...,M^{(1)}$ and $l=2,...,L$. Weights $w$ are usually indendent for each step. Additionally define $z^{(0)}$ to be the input, i.e.

\begin{Equation}[H]
	\centering
	\begin{equation}
	z^{(0)} = x
	\end{equation}
	\label{eq:hommatrix}
\end{Equation}

\noindent For each layer $l \in 1,...,L$ the computation is

\begin{Equation}[H]
	\centering
	\begin{equation}
		z^{(l)}_m = f( (w^{(l)}_m)^T z^{(l-1)} + b^{(l)}_m)
	\end{equation}
	\label{eq:mathmodel}
\end{Equation}

\noindent which can be written as a matrix multiplication:

\begin{Equation}[H]
	\centering
	\begin{equation}
		z^{(l)} = f( W^{(l)} z^{(l-1)} + b^{(l)})
	\end{equation}
	\caption[Forward propagation.]{Function that idetifies input transformation at each step $l$ of the net.}
	\label{eq:activationfun}
\end{Equation}

\noindent The weights $w$ are directed connections between the neurons, e.g. the neurons of layer 2 are connected to the ones of layer 1 by the weights $w^{(2)}_{mn}, m=1,...M^{(1)}, n=1,...,M^{(2)}$. The bias $b$ varies according to the propensity of the neuron to activate, influencing its output. 

\noindent $f()$ is an activation functions and need to be differentiable, because we wish to apply gradient descent training. For the hidden layers of the nework, the activation function must be nonlinear, because multiple linear computations can be collapsed to a single one, therefore in order to gain power from iterative computation we thus need nonlinear steps.

\noindent Many possible activation functions for the hidden layers of a \gls{nn} exist:
\begin{itemize}
	\item Sigmoid, Hyperbolic Tangent: Monotonic, squeeze output to a fixed range
	\item ReLU: "almost linear" (a clipped identity function)
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/old_act_fun}
	\caption[Image of a human neuron.]{Neuron and myelinated axon, with signal flow from inputs at dendrites to outputs at axon terminals. The signal represents a short electrical pulse called 'spike'.}
	\label{fig:old_act_fun}
\end{figure}

\noindent One of the key characteristics of modern deep learning system is to use non-saturated activation function (e.g. ReLU) to replace its saturated counterpart (e.g. sigmoid, tanh). The advantage of using non-saturated activation function lies in two aspects: the first is to solve the so called "exploding/vanishing gradient" \cite[]{bengio1994learning}, in particular on the difficulty of training \gls{rnn} \cite[]{pascanu2013difficulty}, while the second is to accelerate the convergence speed. More sophisticated activation function as the "leaky ReLU" trying to solve the dying ReLU problem \cite[]{LeakyReL95:online}. In contrast to ReLU, in which the negative part is totally dropped, leaky ReLU assigns a noon-zero slope to it. Leaky ReLU and its variants are consistently better than ReLU in \gls{cnn} \cite[]{xu2015empirical}.

\noindent Leaky Rectified linear activation is introduced in acoustic model \cite[]{maas2013rectifier}. Mathematically, it is defined as follows:

\begin{Equation}[H]
	\centering
	\begin{equation}
 		f(x) = 
			\begin{cases}
			\frac{x}{a} & \text{if $x < 0 $} \\
			x & \text{otherwise}
		\end{cases}
	\end{equation}
	\caption[Leaky Rectified linear activation.]{Function that idetifies input transformation at each step $l$ of the net.}
	\label{eq:activationfun}
\end{Equation}

\noindent where $a$ is a fixed parameter in range $(1; +\inf)$. In original paper, the authors suggest to set $a$ to a large number like $100$.

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{images/leaky_relu}
	\caption[Leaky Rectified linear activation.]{Neuron and myelinated axon, with signal flow from inputs at dendrites to outputs at axon terminals. The signal represents a short electrical pulse called 'spike'.}
	\label{fig:leakyrelu}
\end{figure}

\subsection{\gls{nn} Setup for Classification}
\label{nn:nnclassification}
For a classification task with $K$ classes, we use a $K$-idmendional output layer. A sample $x \in R^D$ is classified as belonging to class $k$ if the output neuron $y_k$ has the maximal value:

\begin{Equation}[H]
	\centering
	\begin{equation}
		c^*= \operatorname*{arg\,max}_k  y_k
	\end{equation}
	\label{eq:mathmodelaada}
\end{Equation}

\noindent The problem is that the $arg max$ function is not differentiable. Therefore, this is solved by letting the \gls{nn} output a probability distribution over classes:

\begin{Equation}[H]
	\centering
	\begin{equation}
		y = (y_k)_{k=1,...,K}
		\quad
		\quad
		\quad
		y_k \geq 0,
		\quad
		\sum_{k} y_k = 1
	\end{equation}
	\caption[Forward propagation.]{Function that idetifies input transformation at each step $l$ of the net.}
	\label{eq:activationfun}
\end{Equation}

\noindent The advantage is that we can derive a differentiable measure of the quality of the output on theoretical grounds, using probability theory. In order to make the network output a probability distribution, we take exponentials and normalize. This is the softmax nonlinearity:

\begin{Equation}[H]
	\centering
	\begin{equation}
		S(y) = (\frac{e^{y_1}}{\sum_k e^{y_k}},...,\frac{e^{y_K}}{\sum_k e^{y_k}})
	\end{equation}
	\caption[Forward propagation.]{Function that idetifies input transformation at each step $l$ of the net.}
	\label{eq:activationfun}
\end{Equation}

\noindent In contrast to other activation functinos, it is applied to the full last layer of the \gls{nn}, not to each indipendent component. The hidden layers can have any nonlinear activation function.

\noindent The learning process is structured as a non-convex optimisation problem in which the aim is to minimise a cost function, which measures the distance between a particular solution and an optimal one.

\noindent If we assume a \gls{nn} with softmax output, we can compute the loss by measuring the cross-entropy between the output distribution and the target distribution.

\noindent Encoding the target in one-hot style, e.g. if a sample belongs to class k, the target is:

\begin{Equation}[H]
	\centering
	\begin{equation}
		t=(0,...,0,1,0,...,0)
	\end{equation}
	\caption[Forward propagation.]{Function that idetifies input transformation at each step $l$ of the net.}
	\label{eq:activationfun}
\end{Equation}

\noindent We treat this as a probability distribution: in an ideal world, a perfect hypothesis $y$ would exactly match this $t$, assigning probability $1$ to the correct class, and probability $0$ otherwise. The cross-entropy loss is defined as:

\begin{Equation}[H]
	\centering
	\begin{equation}
		E_{CE} = -\sum_{k}(t_k \, \log y_k)
	\end{equation}
	\caption[Forward propagation.]{Function that idetifies input transformation at each step $l$ of the net.}
	\label{eq:activationfun}
\end{Equation}

\noindent The intuition about the cross-entropy corresponds to the number of additional bits needed to encode the
correct output, given that we have access to the (possibly wrong) prediction of the network. One property of the cross-entropy loss is that is always non-negative. For efficiency and numerical stability, one should merge
softmax loss and cross-entropy criterion into one function:

\begin{Equation}[H]
	\centering
	\begin{equation}
		E_{CE+SM} = -\sum_{k}(t_k \, \log S_k(y))
	\end{equation}
	\caption[Softmax cross entropy.]{To train the network with backpropagation, you need to calculate the derivative of the loss. In the general case, that derivative can get complicated, but using the softmax and the cross entropy loss, that complexity fades away.}
	\label{eq:softmaxcrossentr}
\end{Equation}

\subsection{Optimisation algorithms}
\label{nn:optmalgo}
The choice of optimization algorithms strongly influence the effectiveness of the learning process as they update and calculate the appropriate and optimal values of that model. Specifically if we consider the gradient descent, the most popular optimization strategy used in machine learning, the extent of the update is determined by the learning rate $\lambda$, which guarantees convergence to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces. By the way, there are better optimization as the non linear conjugate gradient \cite[]{conjugategradient}, BFGS, the improved version to decrease memory usage L-BFGS \cite[]{saputro2017limited}, etc... the main advantages are that no need to manually pick $\lambda$ and often are faster than gradient descent, although more complex algorithms. \\

\noindent The optimiser chosen for this thesis project is Adam, an algorithm for first-order gradient-based optimisation of stochastic objective functions, based on adaptive estimates of lower-order moments \cite[]{kingma2017adam}. It is an extension to stochastic gradient descent that has recently seen broader adoption for \gls{dl} applications in computer vision and \gls{nlp}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse

\begin{figure}[htb]
	\centering
	\includegraphics[width=.7\textwidth]{contents/images/ArtificialNeuronModel}
	\caption{Mathematical model of an Artificial Neuron.}
	\label{fig:neuron}
	\vspace{-0.5cm}
\end{figure}



\subsection{Activation functions}
\label{subsec:activationfun}

An activation function is a fundamental component of the model. It allows the 
network to learn non-linear transformations, in order to be able to compute 
non-trivial problems.
In the course of this study, we used two of the most popular activation functions  
in deep learning, the {hyperbolic tangent} (Tanh) \cite[][]{kalman1992tanh} 
and the {sigmoid} \cite[][]{han1995influence}, visualised in Figure 
\ref{fig:activation}.

\paragraph*{Tanh}
The tanh is a zero-centred function, whose range lies between $(-1, 1)$, and its 
output is given by the following formula:
\begin{Equation}[H]
	\centering
	\begin{equation}
	f(x)= \frac{\sinh (x)}{\cosh (x)} = \bigg( \frac{e^x - e^{-x}}{e^x + 
		e^{-x}}\bigg)
	\end{equation}
	\caption{Hyperbolic Tangent Function (Tanh).}
	\label{eq:tanh}
\end{Equation}

\paragraph*{Sigmoid}
The sigmoid models the frequency of the stimuli emitted by an inactive neuron, 
$\sigma(x)=0$, to one fully saturated with the maximum activation frequency, 
$\sigma(x)=1$. Its  output is given by the following formula:
\begin{Equation}[H]
	\centering
	\begin{equation}
	\sigma(x)= \frac{1}{1 + e^{-x}}
	\end{equation}
	\caption{Sigmoid Function.}
	\label{eq:sigmoid}
\end{Equation}

\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.495\textwidth}
			\includegraphics[width=.8\textwidth]{contents/images/sigmoid2}
			\caption{Tanh activation function.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.495\textwidth}
			\includegraphics[width=.8\textwidth]{contents/images/tanh2}
			\caption{Sigmoid activation function.}
		\end{subfigure}
	\end{center}
	\caption{Trends of two non-linear activation functions.}
	\label{fig:activation}
\end{figure}

%\section{Architetture}
%\label{sec:architetture}
%
%I neuroni vengono organizzati in una struttura detta architettura della rete.
%I dati, partendo da un livello iniziale, chiamato layer di input, attraversano i 
%multipli strati interni della rete, gli hidden layer, raggiungendo l'ultimo livello 
%detto layer di output.
%
%Quando i collegamenti tra i neuroni formano una struttura senza cicli si parla di 
%reti \emph{feed-forward} \cite{svozil1997introduction}.
%
%\subsection{Layer fully-connected}
%\label{subsec:fc}
%
%Un’architettura molto comune nelle reti neurali è una struttura ``densa'', che 
%utilizza \emph{layer fully-connected}, in cui tutti i neuroni del livello precedente 
%sono collegati ad ogni neurone dello strato successivo 
%\cite{sainath2015convolutional}.
%
%Lo scopo di un layer completamente connesso è imparare combinazioni non 
%lineari di feature ad alto livello provenienti dal layer precedente. 
%Una struttura di questo tipo è però caratterizzata da un numero di connessioni 
%che cresce molto velocemente, causando un accrescimento del numero di 
%parametri che la rete deve apprendere.
%Questo comporta un aumento del costo computazionale e un alto rischio di 
%overfitting, approfondito nella sezione \ref{subsec:overfitting}.
%
%Per questo motivo questi vengono spesso sostituiti dai layer convoluzionali.

\subsection{Loss functions}
\label{subsec:lossfunctions}
\glsreset{mse}
\glsreset{bce}
The learning process is structured as a non-convex optimisation problem in which 
the aim is to minimise a cost function, which measures the distance between a 
particular solution and an optimal one.

In the course of this study we used two different objective functions, depending 
on the strategy to be adopted: to solve the first task, that can be modelled as a 
regression problem, we used the \gls{mse} \cite[][]{wang2009mean}, while for the 
second, that is a binary classification problem, we used the \gls{bce} 
\cite[][]{gomez2018understanding}.

\paragraph*{Mean Squared Error} 
The \gls{mse} computes the deviation between the values observed $\hat y_i$ and 
those predicted by the network $y_i$, over the number of predictions $n$, as 
shown in Equation \ref{eq:mse}.
\begin{Equation}[!htb]
	\centering
	\begin{equation}
	\mathtt{MSE} = \frac{\sum_{i=1}^n (y_i-\hat y_i)^2}{n}
	\end{equation}
	\caption{Mean Squared Error (MSE) loss function.}
	\label{eq:mse}
\end{Equation}
Formally, this criterion measures the average of squared error between 
predictions and targets, and learns to reduce it by penalising big errors in the 
model predictions.

\paragraph*{Binary Cross Entropy} 
The \gls{bce} is a combination of the sigmoid activation and the \gls{ce}. It sets up 
a binary classification problem between two classes, with the following 
formulation:

\begin{Equation}[!htb]
	\centering
	\begin{equation}
	\mathtt{BCE} = -\frac{1}{n} \sum_{i=1}^n y_i \cdot \log(\hat y_i) + (1-y_i) 
	\cdot \log(1 - \hat y_i)
	\end{equation}
	\caption[Binary Cross Entropy (BCE) loss function.\bigskip]{Binary Cross 
		Entropy 
		(\gls{bce}) loss function \cite[][]{sadowski2016notes}.}
	\label{eq:bce}
\end{Equation}

\noindent
where $\hat y_i$ is the $i$-th scalar value in the model output, $y_i$ is the 
corresponding target value, and $n$ is the number of scalar value in the model 
output\footnote{\url{https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/binary-crossentropy}}.

This loss function should return high values for bad predictions and low values for 
good ones.

\subsection{Optimisation algorithms}
\label{subsec:optimiser}
Optimisation algorithms are needed to minimise the result of a given objective 
function, which depends on the parameters the model has to learn during 
training.
They strongly influence the effectiveness of the learning process as they update 
and calculate the appropriate and optimal values of that model. 
In particular, the extent of the update is determined by the learning rate, which 
guarantees convergence to the global minimum, for convex error surfaces, and to 
a local minimum, for non-convex surfaces.

\paragraph*{Adam}

The optimiser we have chosen for this thesis project is Adam, {an algorithm for 
	first-order gradient-based optimisation of stochastic objective functions, based 
	on adaptive estimates of lower-order moments} \cite[][]{kingma2014adam, 
	loshchilov2017decoupled}. 

%%%%%%%%%%%%%%%%%%%%%%
\fi